{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c200d3-62b5-42b1-a141-fa4329268f10",
   "metadata": {},
   "source": [
    "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/C3669C-2025-01/blob/main/L06/L06.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae3cfc-896c-45e9-a413-971367773caf",
   "metadata": {},
   "source": [
    "# Setup and Installation\n",
    "\n",
    "You can run this Jupyter notebook either on your local machine or run it at Google Colab.\n",
    "\n",
    "* For local machine, it is recommended to install Anaconda and create a new development environment called `c3669c`.\n",
    "* Pip/Conda install the libraries stated below when necessary.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee8581-524f-4e70-9576-9394082abdc4",
   "metadata": {},
   "source": [
    "# <font color='red'>ATTENTION</font>\n",
    "\n",
    "## Google Colab\n",
    "- If you are running this code in Google Colab, **DO NOT** store the API Key in a text file and load the key later from Google Drive. This is insecure and will expose the key.\n",
    "- **DO NOT** hard code the API Key directly in the Python code, even though it might seem convenient for quick development.\n",
    "- You need to enter the API key at python code `getpass.getpass()` when ask.\n",
    "\n",
    "## Local Environment/Laptop\n",
    "- If you are running this code locally in your laptop, you can create a env.txt and store the API key there.\n",
    "- Make sure env.txt is in the same directory of this Jupyter notebook.\n",
    "- You need to install `python-dotenv` and run the Python code to load in the API key.\n",
    "\n",
    "---\n",
    "```\n",
    "%pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('env.txt')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "```\n",
    "---\n",
    "\n",
    "## GitHub/GitLab\n",
    "- **DO NOT** `commit` or `push` API Key to services like GitHub or GitLab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e821256-0629-4b6f-ab0d-6f029aa960a6",
   "metadata": {},
   "source": [
    "# Lesson 06\n",
    "\n",
    "Hugging Face is a cutting-edge technology company that has become a cornerstone in the field of artificial intelligence (AI) and natural language processing (NLP). Founded in 2016, Hugging Face is best known for its open-source contributions, particularly the Transformers library, which has revolutionized how researchers and developers work with state-of-the-art machine learning models. The company’s mission is to democratize AI by making advanced tools and models accessible to everyone, from students and hobbyists to industry professionals.  \n",
    "\n",
    "At its core, Hugging Face provides a platform for sharing, training, and deploying machine learning models, with a strong focus on NLP tasks such as text classification, translation, summarization, and question answering. The platform hosts thousands of pre-trained models, datasets, and tools that enable users to build AI applications without starting from scratch. This has made Hugging Face a go-to resource for anyone interested in exploring or advancing AI technologies.\n",
    "\n",
    "Hugging Face is not just a tool but a vibrant ecosystem that empowers individuals to innovate and collaborate in the rapidly evolving world of AI. As you embark on your journey in AI and machine learning, Hugging Face is a platform that can help you turn theoretical knowledge into practical, impactful solutions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df192547-cacc-44d9-8503-3c9044996617",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "- Go to [Hugging Face](https://huggingface.co/) to sign up for an account.\n",
    "- Create an access token in `Hugging Face`.\n",
    "- You can store the access token in Colab by adding a secret.\n",
    "\n",
    "<img src=\"Colab Secrets.png\" width=\"auto\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649bc2df-0a15-4068-9008-a7a7b99f7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "\n",
    "%pip install --quiet -U transformers\n",
    "%pip install --quiet -U gradio\n",
    "\n",
    "# uncomment to either install\n",
    "# pytorch or tensorflow\n",
    "\n",
    "# if you are running this code in Colab\n",
    "# it is not necessary to install either\n",
    "#%pip install --quiet -U torch\n",
    "#%pip install --quiet -U tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82328e0e-4fc3-44f0-846d-0ccb7a8c9785",
   "metadata": {},
   "source": [
    "# Using Pipeline\n",
    "\n",
    "## Sentiment Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b96bc345-9ad7-47d5-8bc0-75afc88f3480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model card: https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
    "from transformers import pipeline\n",
    "\n",
    "# load a pretrained sentiment analysis_model\n",
    "sentiment_classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77bf2190-ac77-44c0-ab8c-aae7a3eb4827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9997085928916931}]\n"
     ]
    }
   ],
   "source": [
    "text = \"I love using Hugging Face!\"\n",
    "\n",
    "result = sentiment_classifier(text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef69d98-0417-442e-b1e1-d7c599e99431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9995294809341431}]\n"
     ]
    }
   ],
   "source": [
    "text = \"You have to do better in NLP.\"\n",
    "\n",
    "result = sentiment_classifier(text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500aeb08-e96b-44b4-bef5-45678c0744db",
   "metadata": {},
   "source": [
    "## Translation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7930ef75-74d4-4abc-b969-3b9f17286d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original: \n",
      "The Merlion is a famous symbol of Singapore. It has the head of a lion and the body of a fish. The lion represents Singapore's original name, \"Singapura,\" meaning \"Lion City,\" while the fish body shows its history as a fishing village. The Merlion statue is a popular tourist spot!\n",
      "\n",
      "Translation: 狮子代表新加坡原名\"新加坡\"意思是\"狮子城\",而鱼体则显示其作为渔村的历史.\n"
     ]
    }
   ],
   "source": [
    "# model card: https://huggingface.co/facebook/nllb-200-distilled-600M\n",
    "import torch\n",
    "\n",
    "translator = pipeline(\n",
    "    task=\"translation\",\n",
    "    model=\"facebook/nllb-200-distilled-600M\",\n",
    "    torch_dtype=torch.bfloat16 \n",
    ")\n",
    "\n",
    "text = \"\"\"\n",
    "The Merlion is a famous symbol of Singapore. It has the head of a lion and the body of a fish. \\\n",
    "The lion represents Singapore's original name, \"Singapura,\" meaning \"Lion City,\" \\\n",
    "while the fish body shows its history as a fishing village. The Merlion statue is a popular tourist spot!\n",
    "\"\"\"\n",
    "\n",
    "# check https://huggingface.co/facebook/nllb-200-distilled-600M/blob/main/README.md\n",
    "# for language details\n",
    "text_translated = translator(\n",
    "    text,\n",
    "    src_lang=\"eng_Latn\",\n",
    "    tgt_lang=\"zho_Hans\"\n",
    ")\n",
    "\n",
    "print(f\"\\nOriginal: {text}\")\n",
    "print(f\"Translation: {text_translated[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98978c4-a4ad-4991-aad5-ceb65d484fda",
   "metadata": {},
   "source": [
    "# Using the Model Directly\n",
    "\n",
    "Pytorch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b270d82-f5da-44e5-96cd-201f0691f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model card: https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfc4625a-dc5a-4c84-880a-d27cba58e0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment is: Positive\n"
     ]
    }
   ],
   "source": [
    "# Encode text\n",
    "text = \"I love programming!\"\n",
    "tokens = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "label_ids = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "labels = ['Negative', 'Positive']\n",
    "label = labels[label_ids]\n",
    "\n",
    "print(f\"The sentiment is: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "573c1755-ca3d-4772-84d1-6e018cc4cbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment is: Negative\n"
     ]
    }
   ],
   "source": [
    "# Encode text\n",
    "text = \"I hate programming!\"\n",
    "tokens = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "label_ids = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "labels = ['Negative', 'Positive']\n",
    "label = labels[label_ids]\n",
    "\n",
    "print(f\"The sentiment is: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132e3ef6-c769-4345-b347-a9384c147971",
   "metadata": {},
   "source": [
    "# Fill Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da1b9681-0a84-43a0-b944-59dec11f37fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.052928801625967026,\n",
       "  'token': 2535,\n",
       "  'token_str': 'role',\n",
       "  'sequence': \"hello i ' m a role model.\"},\n",
       " {'score': 0.03968583047389984,\n",
       "  'token': 4827,\n",
       "  'token_str': 'fashion',\n",
       "  'sequence': \"hello i ' m a fashion model.\"},\n",
       " {'score': 0.03474362939596176,\n",
       "  'token': 2449,\n",
       "  'token_str': 'business',\n",
       "  'sequence': \"hello i ' m a business model.\"},\n",
       " {'score': 0.03462280333042145,\n",
       "  'token': 2944,\n",
       "  'token_str': 'model',\n",
       "  'sequence': \"hello i ' m a model model.\"},\n",
       " {'score': 0.01814517006278038,\n",
       "  'token': 11643,\n",
       "  'token_str': 'modeling',\n",
       "  'sequence': \"hello i ' m a modeling model.\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model card: https://huggingface.co/distilbert/distilbert-base-uncased\n",
    "unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n",
    "\n",
    "unmasker(\"Hello I'm a [MASK] model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e88526-91f8-4ab3-88d5-4e2d0fde36fd",
   "metadata": {},
   "source": [
    "# Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb2ae4e2-8440-4151-84d2-b35d8544ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model card: https://huggingface.co/facebook/bart-large-cnn\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Source: https://en.wikipedia.org/wiki/Quantum_computing\n",
    "Article = \"\"\"A quantum computer is a computer that exploits quantum mechanical phenomena. \n",
    "On small scales, physical matter exhibits properties of both particles and waves, and quantum computing leverages this behavior using specialized hardware. \n",
    "Classical physics cannot explain the operation of these quantum devices, and a scalable quantum computer could perform some calculations exponentially faster than any modern \"classical\" computer. \n",
    "Theoretically a large-scale quantum computer could break some widely used encryption schemes and aid physicists in performing physical simulations; \n",
    "however, the current state of the art is largely experimental and impractical, with several obstacles to useful applications.\n",
    "\n",
    "The basic unit of information in quantum computing, the qubit (or \"quantum bit\"), serves the same function as the bit in classical computing. \n",
    "However, unlike a classical bit, which can be in one of two states (a binary), a qubit can exist in a superposition of its two \"basis\" states, \n",
    "a state that is in an abstract sense \"between\" the two basis states. When measuring a qubit, the result is a probabilistic output of a classical bit. \n",
    "If a quantum computer manipulates the qubit in a particular way, wave interference effects can amplify the desired measurement results. \n",
    "The design of quantum algorithms involves creating procedures that allow a quantum computer to perform calculations efficiently and quickly.\n",
    "\n",
    "Quantum computers are not yet practical for real work. Physically engineering high-quality qubits has proven challenging. \n",
    "If a physical qubit is not sufficiently isolated from its environment, it suffers from quantum decoherence, introducing noise into calculations. \n",
    "National governments have invested heavily in experimental research that aims to develop scalable qubits with longer coherence times and lower error rates. \n",
    "Example implementations include superconductors (which isolate an electrical current by eliminating electrical resistance) and ion traps (which confine a single atomic particle using electromagnetic fields).\n",
    "\n",
    "In principle, a classical computer can solve the same computational problems as a quantum computer, given enough time. \n",
    "Quantum advantage comes in the form of time complexity rather than computability, \n",
    "and quantum complexity theory shows that some quantum algorithms are exponentially more efficient than the best-known classical algorithms. \n",
    "A large-scale quantum computer could in theory solve computational problems unsolvable by a classical computer in any reasonable amount of time. \n",
    "This concept of extra ability has been called \"quantum supremacy\". \n",
    "While such claims have drawn significant attention to the discipline, near-term practical use cases remain limited.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21057ce1-707d-4c83-98c0-c8da7214d3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: A quantum computer is a computer that exploits quantum mechanical phenomena. A large-scale quantum computer could in theory solve computational problems unsolvable by a classical computer in any reasonable amount of time. The current state of the art is largely experimental and impractical, with several obstacles to useful applications.\n"
     ]
    }
   ],
   "source": [
    "Output = summarizer(Article, max_length=200, min_length=50, do_sample=False)\n",
    "\n",
    "print(f\"Summary: {Output[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3377ff65-9eae-4d46-8ff5-aa91a868a387",
   "metadata": {},
   "source": [
    "# Question and Answering\n",
    "\n",
    "Using `Roberta` model as a Question and Answering chatbot. We will provide context and query the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ac3d61e-2bc2-4d43-9160-9be4885c1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model card: https://huggingface.co/deepset/roberta-base-squad2\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "nlp = pipeline(\n",
    "    'question-answering', \n",
    "    model=model_name, \n",
    "    tokenizer=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cc12644-206d-4c86-a608-7861ed571007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koay_seng_tian\\AppData\\Local\\anaconda3\\envs\\c3669c\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the current (2025) Enhanced Retirement Sum (ERS) in dollar?\n",
      "Answer: $426,000\n"
     ]
    }
   ],
   "source": [
    "QA_Input = {\n",
    "    'question' : 'What is the current (2025) Enhanced Retirement Sum (ERS) in dollar?',\n",
    "    'context' : \"\"\"The ERS is increased yearly. For members 55 and above who wish to have higher monthly payouts in retirement, they can top up their Retirement Account up to the current ERS. To find out how much you can top up, log in to your Retirement dashboard.\n",
    "\n",
    "From 2025, the ERS was raised from 3 times the Basic Retirement Sum (BRS) to 4 times to enable members to voluntarily commit more savings for even higher payouts. \n",
    "\n",
    "This allows members to get higher CPF LIFE monthly payouts of up to $3,300 for life from age 65 through CPF top-ups. \n",
    "    \n",
    "Here are the ERS sums from years 2025 to 2027 to enable you to plan your top-ups:\n",
    "    2025: $426,000\n",
    "    2026: $440,800\n",
    "    2027: $456,400\"\"\"\n",
    "}\n",
    "\n",
    "response = nlp(question=QA_Input['question'], context=QA_Input['context'])\n",
    "\n",
    "print(f\"Question: {QA_Input['question']}\")\n",
    "print(f\"Answer: {response['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad9690-6b8c-4aa4-9874-555bf78d9754",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43943d2-4aed-47e3-9ea6-ab584bdbc8ff",
   "metadata": {},
   "source": [
    "# Deployment (Local)\n",
    "\n",
    "Deploying large language models (LLMs) from Hugging Face has become incredibly accessible thanks to tools like Streamlit and Gradio. These frameworks allow you to quickly create interactive web applications for showcasing and interacting with AI models, making them ideal for students, researchers, and developers.\n",
    "\n",
    "Below is a screen shot of the deployment done on local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b96f9-b1b8-40fc-8155-13a32b68b12f",
   "metadata": {},
   "source": [
    "<img src=\"Gradio Screenshot 01.png\" width=\"auto\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94474a92-0481-4870-bb2e-03472bca85df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9142da-aee7-4c5c-8ad7-783170cb4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\n",
    "   'question-answering', \n",
    "    model=model_name, \n",
    "    tokenizer=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3aef44-0eb3-4394-811d-590f4afecc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, context):\n",
    "    QA_Input = {\n",
    "        'question': question,\n",
    "        'context': context      \n",
    "    }\n",
    "\n",
    "    response = nlp(question=question, context=context)\n",
    "    result = response['answer']\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c7d73b-d726-425f-a3be-090e345b2c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=generate_answer, \n",
    "    \n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Question:\", placeholder=\"Enter your question here...\"),\n",
    "        gr.Textbox(label=\"Context:\", placeholder=\"Enter the context here...\")\n",
    "    ],\n",
    "\n",
    "    outputs=gr.Textbox(label=\"Answer:\"),\n",
    "    \n",
    "    title=\"Question and Answering\",\n",
    "    description=\"Enter a question and a context and the system will provide an answer.\"\n",
    ")\n",
    "\n",
    "iface.launch(server_name=\"0.0.0.0\", server_port=7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f81dac-345e-4821-a122-9985309a1a75",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89703861-1433-4b09-9a45-10ceda3ccc45",
   "metadata": {},
   "source": [
    "# Deployment (Hugging Face Space)\n",
    "\n",
    "Refer to lesson PowerPoint for instructions on how to setup the Gradio app to run in Hugging Face Space.\n",
    "\n",
    "<img src=\"Gradio Screenshot HFS 99.png\" width=\"auto\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b11117-03c6-4392-9130-203933519a68",
   "metadata": {},
   "source": [
    "You will need the files:\n",
    "- [app.py](https://github.com/koayst-rplesson/C3669C-2025-01/blob/main/L06/app.py)\n",
    "- [requirements.txt](https://github.com/koayst-rplesson/C3669C-2025-01/blob/main/L06/requirements.txt)\n",
    "- [Dockerfile](https://github.com/koayst-rplesson/C3669C-2025-01/blob/main/L06/Dockerfile)\n",
    "\n",
    "The Gradio app is hosted in Hugging Face Space using the `Docker container` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85a80f-0a57-4783-b1e7-4c1e1fda7e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
