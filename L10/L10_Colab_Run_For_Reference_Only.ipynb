{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a07f6d73-8bbb-4832-b0cd-a3f30aba009d",
      "metadata": {
        "id": "a07f6d73-8bbb-4832-b0cd-a3f30aba009d"
      },
      "source": [
        "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/C3669C-2025-01/blob/main/L10/L10_Colab_Run_For_Reference_Only.ipynb)\n",
        "\n",
        "<font color='red' size=\"5\">\n",
        "<u>Do not run the code in this notebook. It is used as a reference or for comparison when you run L10.ipynb</u> in Google Colab.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae6ff1dc-2734-497d-be36-279f2dc76beb",
      "metadata": {
        "id": "ae6ff1dc-2734-497d-be36-279f2dc76beb"
      },
      "source": [
        "# Setup and Installation\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\" width=\"20%\" height=\"auto\"/>\n",
        "\n",
        "You **MUST** run this Jupyter notebook at Google Colab.  We will using **unsloth** library. **Unsloth** makes finetuning large language models like Llama-3 2X faster and using 70% less memory with no degradation in accuracy.\n",
        "\n",
        "[Reference: Unsloth documentation](https://docs.unsloth.ai/)\n",
        "\n",
        "The following Python code was modified/adapted from Unsloth.\n",
        "\n",
        "### References\n",
        "- [github/unsloth](https://github.com/unslothai/unsloth)\n",
        "- [Unsloth documentation](https://docs.unsloth.ai/)\n",
        "- [huggingface/unsloth](https://huggingface.co/unsloth)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35db9108-5cd7-4d05-ab39-6ca01fa370d2",
      "metadata": {
        "id": "35db9108-5cd7-4d05-ab39-6ca01fa370d2"
      },
      "source": [
        "### Google Colab\n",
        "- At Google Colab, Go to `Runtime` > `Change runtime type` and select `T4 GPU`. Or if you are lucky, choose `A100 GPU`.\n",
        "- Using T4 GPU is free with limited access time\n",
        "- If you have Colab+ subscription, you can select A100 GPU.\n",
        "\n",
        "<img src=\"colab-01.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f63cfe5f-e947-458c-82f6-d6a2ca4fda98",
      "metadata": {
        "id": "f63cfe5f-e947-458c-82f6-d6a2ca4fda98"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install unsloth\n",
        "\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "16e23847-01ec-41ad-9879-153f503cc11a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16e23847-01ec-41ad-9879-153f503cc11a",
        "outputId": "4a539b9f-f3bd-483d-d74a-50517d63f237"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None          # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True   # Use 4bit quantization to reduce memory usage. Can be False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e271b221-9742-42ed-9192-69db51006784",
      "metadata": {
        "id": "e271b221-9742-42ed-9192-69db51006784"
      },
      "outputs": [],
      "source": [
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5405f52d-95a7-4ba2-8d9f-41d7adbb5b5d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281,
          "referenced_widgets": [
            "15690602b8bf4aa8aae882bd19944bd3",
            "c46100051dd94f83b2cae29140e16b9e",
            "65aa45e9991c4b68adceade7778a062a",
            "b7e93de49dce421ab9a72563ee8b8dbe",
            "09ff7cafd9b649a582f08f3b2b7c713a",
            "8dd2ea172d7444d1af6afd915758fe05",
            "bfcb9a03f9b440ff9bf6e288d0b80fa2",
            "b07aa52374fc4ae392defbd39043cc96",
            "5b8636091e8e408499ae727a86d974db",
            "d5fd154348484b108fce253e39e2bee9",
            "91b2cccfccbd4b70b31790ca78066bd3",
            "2310b75fd03b4fa283e885d9d735004a",
            "8736f5794a2648c6a93b78e30ca29074",
            "23373cdc88e54976b779ab1aaeee562d",
            "59d2e572041b4506b5287bbfd1e5830a",
            "c5877c22d05f442aa4814675e4afc390",
            "b19c4116481045888d69a92013fbab26",
            "392efa534d7148a3a25f3dfa1208fcdd",
            "5404fb711ef14519a3356fa311b36491",
            "d9f6dbaf37ec47138f77c97aa535d45d",
            "ba6c25d4a6494ff995811cacc27db5db",
            "67ef1405f71a410faade2f0335645bcc",
            "7c51cb1fd4db4218963d115812b98648",
            "b347f0d0d61c4dceafba80f67792fcf3",
            "3f984f6d95294ab59c18fb700520e8c4",
            "f275072484d94fa798d71f45eb44a53a",
            "a3b489b6d925490a851daa998a57db69",
            "06b319af25214307903b3be1032226c7",
            "f9491f11726648baaa45e8008d695322",
            "d5373b72c67a43e49e4a0c5edfb5562c",
            "9e178459201a40408701ba7a1379584f",
            "1d5ab7bf7e2b450784d21cbd352f57a1",
            "083455a1a8a544e4b6eeb48865658002",
            "1ef8073ce5474b51a8be68af9c003772",
            "de2aa937b6d04f5c88c997da49732062",
            "3f824062821c42068c69f56902a20d2b",
            "aede86c1e96248d0ad2819b5715247f3",
            "27aaed6ad5d84003a319fb4b81664801",
            "14865c5f724f41bc916ac266ef2f9bf1",
            "58f4cc5cfe53441ca1dd98f151562e68",
            "3fb04c474704468d97b3eb4b62cc0fdb",
            "f4a86c7a4c2d48eb961a52eb125e48a6",
            "44a316191646410d953fbba80670a12f",
            "46433118cfb94128a72280bcda7533fa",
            "16d9a68335c44a5d9bbce703485fc795",
            "e4aced235b964b33bb56046b6a749719",
            "05a4441154d84d65952734f1072fe796",
            "80cec5ddb1f64795b95fa1a14e92212e",
            "e73cfe6c57234df698a2ee61d257a9a1",
            "9dbab1a9a489439fab3d8cb2a0cba1db",
            "cda80f6caa19434bb041cd0b526b6e09",
            "56e956d6c45e4420b5291220374c63e6",
            "045011f9af7f48b5a198907464073b11",
            "0925b757296849768aea35ffa4af1de9",
            "cc6092e08772472ea926dec9120c6e0d"
          ]
        },
        "id": "5405f52d-95a7-4ba2-8d9f-41d7adbb5b5d",
        "outputId": "d55edfb8-08e9-4ccc-992d-db719a5cb650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2024.11.10: Fast Llama patching. Transformers:4.46.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15690602b8bf4aa8aae882bd19944bd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2310b75fd03b4fa283e885d9d735004a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c51cb1fd4db4218963d115812b98648",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ef8073ce5474b51a8be68af9c003772",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16d9a68335c44a5d9bbce703485fc795",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# if you happened to use one of the gated models (for example: meta/llama 3.2), you need to go to huggingface to apply for permission to access.\n",
        "# Possible error message looks like this:\n",
        "#    Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list.\n",
        "#    Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.`\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e1e3fe1-3c9b-4d4d-b49f-4007107aaf53",
      "metadata": {
        "id": "8e1e3fe1-3c9b-4d4d-b49f-4007107aaf53"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "02bc4fb5-d612-4851-962c-fb95261f4c19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02bc4fb5-d612-4851-962c-fb95261f4c19",
        "outputId": "eb6ff1a5-bc2e-4cf1-f21c-5470ad29ece3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2024.11.10 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "\n",
        "    target_modules = [\"q_proj\",\n",
        "                      \"k_proj\",\n",
        "                      \"v_proj\",\n",
        "                      \"o_proj\",\n",
        "                      \"gate_proj\",\n",
        "                      \"up_proj\",\n",
        "                      \"down_proj\",],\n",
        "\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcb6982f-4090-4995-a95e-cdcc47c8ce36",
      "metadata": {
        "id": "dcb6982f-4090-4995-a95e-cdcc47c8ce36"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Preparation\n",
        "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
        "\n",
        "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
        "\n",
        "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
        "\n",
        "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e822a697-6779-4ed3-938c-98174cf120ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "b61d9b8efff44a73adc96144d6e49772",
            "16b9816fc9694d2dba11073774735b7b",
            "485c472dea534eafa4eab19ad5370b5b",
            "53c93af08d9c4cb088dea1416975da27",
            "23dfcd826392456ebd72e35d59807e83",
            "68dceb6f00974e45b96c7bd88321ecd9",
            "6b56c47a0f1141dfb56fe2a7f329832b",
            "ef72c96ac24b40fc84745245db6e0ef5",
            "574697a9189b4b9990ee6c0dd310dd46",
            "00cd34f94fcc42a1b175ecaf9211a2fb",
            "7d163389f25b4d328afcbf388b23cd63",
            "ae37d85a135048f2bfcc8041f2e3b5a8",
            "8a99a3445dfa415486288a20a8ba3fd6",
            "95ddbbaaa62341a7889133bcf8927762",
            "154bd4a41e5348ef94d334d38b9e2c66",
            "4ef971d3d8524fea9592362b66e050b7",
            "172a93f459db4cc5accf702992eaebce",
            "66c39adee94a40dd9d1db96ff8cbaee6",
            "5d573953d3e54f0f9cdf1bbacac56a0b",
            "318c28a996e343f68b184198d9741c5b",
            "dc881bdda8c84496977f996f586c5066",
            "f4ccd5c5f8d8483e9150ce1d9d914cd3",
            "499613e8a7ff49d4b767dd6b879d31c3",
            "bad46ca2b9a14c55944e44a815be9916",
            "b1151c366cd346cc8c7a5a8e0cb6fc2a",
            "637b986c300840d0979a51ee66d19481",
            "b3bf9aa3c8ee41ea948de8cf59c26be7",
            "c1dded05b9914913996455406337d605",
            "b0d23ad7ebe642819b40316d3fb2e26d",
            "52ecd5e4542445c6b480edf4a95022ea",
            "47b3bdf7ed4c4956bee202042ada1570",
            "ebac1884440a4e59b159ae50d2e909c8",
            "4d7df1b3458f434db1c74cc579c6c808",
            "12d58e14c4b74e669b60eccebd9a3389",
            "e649886da759452e9284035a83dd239d",
            "8a5828c44f974239824b74f16abb5566",
            "641f0b7828224b7a80bfcaa0206a24e8",
            "0f49b994d3b64ecaa0c0a186001b813f",
            "a76f9d8b7658464b82b881e7a4ccdc8a",
            "9ff65af36a544605a7ea520f7bfed260",
            "ffe9de2ebd724bd58bdefba17dac3b0a",
            "22eec714287044dc8d3cd34eed7ccb77",
            "fc812cd2474b4be5aefda19ecd118088",
            "25ff7041a5504ed4878fc39747fe828d"
          ]
        },
        "id": "e822a697-6779-4ed3-938c-98174cf120ac",
        "outputId": "06da335e-33bf-49c2-908a-45b74b49440d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b61d9b8efff44a73adc96144d6e49772",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae37d85a135048f2bfcc8041f2e3b5a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "alpaca_data_cleaned.json:   0%|          | 0.00/44.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "499613e8a7ff49d4b767dd6b879d31c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/51760 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12d58e14c4b74e669b60eccebd9a3389",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/51760 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "903ac708-f84e-4595-8b65-c2c5baf3eef0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "903ac708-f84e-4595-8b65-c2c5baf3eef0",
        "outputId": "0a7cbb71-48a0-49a1-b2ab-5905679e92be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['output', 'input', 'instruction', 'text'],\n",
            "    num_rows: 51760\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d3325f3d-844d-4d8b-ae43-069b59f5d904",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3325f3d-844d-4d8b-ae43-069b59f5d904",
        "outputId": "e84710d5-e8f2-4af2-b129-afa901253962"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:\n",
            "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
            "\n",
            "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
            "\n",
            "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.\n",
            "\n",
            "Input:\n",
            "\n",
            "\n",
            "Instruction:\n",
            "Give three tips for staying healthy.\n",
            "\n",
            "Text:\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Give three tips for staying healthy.\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
            "\n",
            "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
            "\n",
            "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<|end_of_text|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Output:\\n{dataset[0]['output']}\\n\")\n",
        "print(f\"Input:\\n{dataset[0]['input']}\\n\")\n",
        "print(f\"Instruction:\\n{dataset[0]['instruction']}\\n\")\n",
        "print(f\"Text:\\n{dataset[0]['text']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a15d7ee-cea0-4272-b17d-d834126277ef",
      "metadata": {
        "id": "7a15d7ee-cea0-4272-b17d-d834126277ef"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "aa217556-7984-48cb-b122-5208878bb6ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "f0089e9d111e49d688a5ecae23af4338",
            "48ef463e048847c99582ca5ecbc2a241",
            "cc8dae60bed94b7fb23f685817956ff8",
            "652b33eb84d84d098dbc9bb2018d897b",
            "0a75f268c89a42b09d3637bc35e7a639",
            "a4892055309545038ab3bec2780f0dbc",
            "a975897880104d8f881388d129c17a5b",
            "0d8bc006caa649ae981ec6f06e9a614f",
            "1c9532afd6484fe7a9ebc0680fd26b56",
            "b5d1aa4351bd4f2680bbc972f1a8ed4d",
            "9daf09c5b29e4cea9ad5ac83e77d927e"
          ]
        },
        "id": "aa217556-7984-48cb-b122-5208878bb6ca",
        "outputId": "5e443f69-988e-4c95-bb94-73f929e4e967"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0089e9d111e49d688a5ecae23af4338",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/51760 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3c3f0847-7d17-46b3-b96b-f454b6c1f1a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c3f0847-7d17-46b3-b96b-f454b6c1f1a0",
        "outputId": "021a64f3-1344-41ac-bb08-7d56dd33c78b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.748 GB.\n",
            "6.004 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ebbb4f94-8023-452e-98b5-2098cc51bc36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ebbb4f94-8023-452e-98b5-2098cc51bc36",
        "outputId": "d321289f-6fd7-4788-d64b-d82c2e31f7c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 51,760 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 60\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 07:42, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.586600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.115000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.672800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.862700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.678700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.490000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.080800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.269500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.143700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.121800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.904300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.997200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.919100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.079300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.900200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.863400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.991900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.338200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.033000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.872000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.923500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.989000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.990500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.995200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.071800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.045100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.040300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.926100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.927000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.852100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.861400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.873600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.009000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.862200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.999100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.879600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.819300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.733800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.060600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.180600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.901100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.967800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.878700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.911200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.969000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.941000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.815900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.239800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.843700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.059400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.038400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.898400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.006100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.174400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.794400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.998800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.879900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.771100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.837000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.885000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 5min 39s, sys: 2min 15s, total: 7min 54s\n",
            "Wall time: 8min 12s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c0ee1f4a-98d6-493e-a437-bc0dcffaa18c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0ee1f4a-98d6-493e-a437-bc0dcffaa18c",
        "outputId": "967538d7-c38e-49e7-c9df-5b31eacbc823"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "488.2453 seconds used for training.\n",
            "8.14 minutes used for training.\n",
            "Peak reserved memory = 8.0 GB.\n",
            "Peak reserved memory for training = 1.996 GB.\n",
            "Peak reserved memory % of max memory = 54.245 %.\n",
            "Peak reserved memory for training % of max memory = 13.534 %.\n"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6118073-c1e6-465a-a116-cc39b92f4e3c",
      "metadata": {
        "id": "c6118073-c1e6-465a-a116-cc39b92f4e3c"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "27b4c5d5-5ee0-45bb-9865-468a8b4dd8c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27b4c5d5-5ee0-45bb-9865-468a8b4dd8c8",
        "outputId": "e1b61489-5d86-4e60-94b9-1cf0d6b56bd4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nContinue the fibonnaci sequence.\\n\\n### Input:\\n1, 1, 2, 3, 5, 8\\n\\n### Response:\\n13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # instruction\n",
        "        \"1, 1, 2, 3, 5, 8\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "451dc78c-ed93-4979-9b4c-9bfce9ee6610",
      "metadata": {
        "id": "451dc78c-ed93-4979-9b4c-9bfce9ee6610"
      },
      "source": [
        "You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "911e502e-1758-46ee-9422-8a0fffdc2ce2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "911e502e-1758-46ee-9422-8a0fffdc2ce2",
        "outputId": "bbd1deb9-a8c4-45c1-bfee-55272d361aab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "13, 21, 34, 55, 89, 144<|end_of_text|>\n"
          ]
        }
      ],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # instruction\n",
        "        \"1, 1, 2, 3, 5, 8\", # input\n",
        "        \"\", # output/response - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4193f44d-98da-4825-b263-e260d9519e83",
      "metadata": {
        "id": "4193f44d-98da-4825-b263-e260d9519e83"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0ce2bc6f-c82d-4847-b2b9-179987495256",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ce2bc6f-c82d-4847-b2b9-179987495256",
        "outputId": "282021b3-340e-45d2-e060-5eb3c3ad2f4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# connect to google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f456cc49-9380-4dd8-b650-a3ba2d1773c6",
      "metadata": {
        "id": "f456cc49-9380-4dd8-b650-a3ba2d1773c6"
      },
      "outputs": [],
      "source": [
        "path_to_saved_model = \"/content/drive/MyDrive/Colab Notebooks/c3669c/L06/lora_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "71435773-e19e-45b3-bd4e-8fee9a94359f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71435773-e19e-45b3-bd4e-8fee9a94359f",
        "outputId": "1d9d3cfe-366d-4d09-971a-9bf0e5bed191"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Colab Notebooks/c3669c/L06/lora_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/c3669c/L06/lora_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/c3669c/L06/lora_model/tokenizer.json')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(path_to_saved_model) # Local saving\n",
        "tokenizer.save_pretrained(path_to_saved_model)\n",
        "\n",
        "# you can push to huggingface if you have an account\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "9fe03aee-35d3-4249-9aca-38a2ada511da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fe03aee-35d3-4249-9aca-38a2ada511da",
        "outputId": "5705c10a-3236-454a-8478-cbc17652549e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 180762\n",
            "-rw------- 1 root root       734 Dec  2 08:20 adapter_config.json\n",
            "-rw------- 1 root root 167832240 Dec  2 08:20 adapter_model.safetensors\n",
            "-rw------- 1 root root      5108 Dec  2 08:20 README.md\n",
            "-rw------- 1 root root       459 Dec  2 08:20 special_tokens_map.json\n",
            "-rw------- 1 root root     50570 Dec  2 08:20 tokenizer_config.json\n",
            "-rw------- 1 root root  17209920 Dec  2 08:20 tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "# check the files are save to directory lora_model\n",
        "\n",
        "!ls -al \"/content/drive/MyDrive/Colab Notebooks/c3669c/L06/lora_model\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8afc5ee-1c94-4e01-84f1-63a932e8fd9d",
      "metadata": {
        "id": "c8afc5ee-1c94-4e01-84f1-63a932e8fd9d"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ae2c67e5-d0cc-407d-8ece-4fcc50fbe617",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae2c67e5-d0cc-407d-8ece-4fcc50fbe617",
        "outputId": "6a3045a1-43a2-4640-945d-884c4c2eb8ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2024.11.10: Fast Llama patching. Transformers:4.46.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = path_to_saved_model, # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "6f6b1c49-0668-4787-89e7-ec7396aead14",
      "metadata": {
        "id": "6f6b1c49-0668-4787-89e7-ec7396aead14"
      },
      "outputs": [],
      "source": [
        "# alpaca_prompt = You MUST copy from above!\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c4179207-85ab-41ba-8029-0b11ed3252ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4179207-85ab-41ba-8029-0b11ed3252ce",
        "outputId": "14e6f288-3251-4c83-8762-b5d22f5a28a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is a famous tall tower in Paris?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "One of the most famous and iconic tall towers in Paris is the Eiffel Tower. Standing at 324 meters (1,063 feet) tall, this wrought iron tower is a symbol of the city and a must-see attraction for tourists from all over the world.<|end_of_text|>\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"What is a famous tall tower in Paris?\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "808bec63-ca41-409a-88a5-e49ac0740336",
      "metadata": {
        "id": "808bec63-ca41-409a-88a5-e49ac0740336"
      },
      "source": [
        "### Cleanup\n",
        "\n",
        "If you don't need the models anymore, remember to go to `path_to_saved_model` and delete the directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ab38523-51e0-4402-a996-5f500d3c0af6",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}