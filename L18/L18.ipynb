{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c200d3-62b5-42b1-a141-fa4329268f10",
   "metadata": {},
   "source": [
    "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/C3669C-2025-01/blob/main/L18/L18.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae3cfc-896c-45e9-a413-971367773caf",
   "metadata": {},
   "source": [
    "# Setup and Installation\n",
    "\n",
    "You can run this Jupyter notebook either on your local machine or run it at Google Colab.\n",
    "\n",
    "* For local machine, it is recommended to install Anaconda and create a new development environment called `c3669c`.\n",
    "* Pip/Conda install the libraries stated below when necessary.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee8581-524f-4e70-9576-9394082abdc4",
   "metadata": {},
   "source": [
    "# <font color='red'>ATTENTION</font>\n",
    "\n",
    "## Google Colab\n",
    "- If you are running this code in Google Colab, **DO NOT** store the API Key in a text file and load the key later from Google Drive. This is insecure and will expose the key.\n",
    "- **DO NOT** hard code the API Key directly in the Python code, even though it might seem convenient for quick development.\n",
    "- You need to enter the API key at python code `getpass.getpass()` when ask.\n",
    "\n",
    "## Local Environment/Laptop\n",
    "- If you are running this code locally in your laptop, you can create a env.txt and store the API key there.\n",
    "- Make sure env.txt is in the same directory of this Jupyter notebook.\n",
    "- You need to install `python-dotenv` and run the Python code to load in the API key.\n",
    "\n",
    "---\n",
    "```\n",
    "%pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('env.txt')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "```\n",
    "---\n",
    "\n",
    "## GitHub/GitLab\n",
    "- **DO NOT** `commit` or `push` API Key to services like GitHub or GitLab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e821256-0629-4b6f-ab0d-6f029aa960a6",
   "metadata": {},
   "source": [
    "# Lesson 18\n",
    "\n",
    "## Techniques to improve RAG Performance\n",
    "- `Better Embeddings`: Use high-quality embedding models (e.g., OpenAIâ€™s text-embedding-ada-002, Cohere, or BGE) to improve the semantic relevance of retrieved documents.\n",
    "- `Hybrid Search`: Combine dense (vector) search with sparse methods like BM25 to balance precision and recall.\n",
    "- `Query Expansion`: Reformulate queries using LLMs or keyword expansion to improve retrieval accuracy.\n",
    "- `Re-ranking`: Apply a re-ranker (e.g., Cohere Rerank, Cross-Encoders) to reorder retrieved documents based on relevance.\n",
    "\n",
    "The list is non-exhaustive and new techniques are being discovered. In this notebook, we will explore `re-ranking`. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae7c2b-389d-45cb-8f8e-4bfe037cc3ae",
   "metadata": {},
   "source": [
    "Reference: [LangChain Cohere Reranker](https://python.langchain.com/docs/integrations/retrievers/cohere-reranker/)\n",
    "\n",
    "## Cohere ReRanker\n",
    "This notebook use Cohere's rerank endpoint in a retriever. You are required to get an API key from Cohere.\n",
    "\n",
    "- Sign up at [Cohere Dashboard](https://dashboard.cohere.com/welcome/login) to apply an API key.\n",
    "- Sign up at [Hugging Face](https://huggingface.co/) to apply an API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649bc2df-0a15-4068-9008-a7a7b99f7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "\n",
    "%pip install --quiet -U pypdf\n",
    "%pip install --quiet -U faiss-cpu\n",
    "\n",
    "%pip install --quiet -U langchain-community\n",
    "%pip install --quiet -U langchain-huggingface\n",
    "%pip install --quiet -U langchain-openai\n",
    "%pip install --quiet -U langchain-cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82328e0e-4fc3-44f0-846d-0ccb7a8c9785",
   "metadata": {},
   "source": [
    "## Run either option 1 or option 2 to set the API keys and Hugging Face token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96bc345-9ad7-47d5-8bc0-75afc88f3480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1\n",
    "# Run the code if you DIDN'T setup secrets in Google Colab\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# setup the OpenAI API Key\n",
    "# setup the Cohere API Key\n",
    "# setup the Hugging Face Token\n",
    "\n",
    "# get API keys ready and enter them when ask\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API key: \")\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API key: \")\n",
    "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Hugging Face token: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d6c5d-f6fe-4d12-abbd-f97b8b6cec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2\n",
    "# Run the code if you setup secrets in Google Colab\n",
    "\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "os.environ[\"COHERE_API_KEY\"] = userdata.get(\"COHERE_API_KEY\")\n",
    "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adca1a-d522-4e39-9f36-07cc2b808ff1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d0587-eb0b-4212-a17a-586436b8321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere.rerank import CohereRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78030017-eff8-401e-baad-284ef612a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for printing docs\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join( [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)] )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be885c-03d5-498b-bb09-920cbbf230bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the zip file from GitHub repository\n",
    "# Introduction-to-Management-Studies.pdf is in the zip file\n",
    "\n",
    "!wget https://github.com/koayst-rplesson/SDGAI_LLMforGenAIApp_Labs/raw/refs/heads/main/L15/Introduction-to-Management-Studies.zip\n",
    "!unzip Introduction-to-Management-Studies.zip\n",
    "!ls -al\n",
    "\n",
    "# make sure you can see \"Introduction-to-Management-Studies.pdf\" in the directory listing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c8333-4179-475a-bbcd-dd982fb1fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the PDF file from the current directory\n",
    "pdf_folder_path = \".\"\n",
    "\n",
    "loader = PyPDFDirectoryLoader(pdf_folder_path)\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))\n",
    "\n",
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe560cc6-5d7f-4a03-8b31-69a34d3b7327",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4b0150-ec19-4f60-bbb6-526a5ad24927",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237e6643-aad0-4d37-82d9-9b868bf44364",
   "metadata": {},
   "source": [
    "## Set up the base vector store (FAISS) retriever\n",
    "\n",
    "Initialise a simple vector store retriever and store document (in chunks). We can set up the retriever to retrieve a high number (20) of docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b06be1b-7cf7-4a3d-86dd-9da4b22341e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# retrieve first top 20 chunks\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67955fb-4997-4cd3-9b6e-a464ddf0ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"According to Kelly and Williams what is ethics?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c65ea4-a01c-4398-9668-c85a63114293",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo-16k\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=model_name, temperature=0.1)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1916a67-d862-4fce-8e92-bae2a39a1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "response = qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00038264-9588-449e-8177-481f135f4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query = {response['query']}\")\n",
    "print(f\"Result = {response['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486fe00d-3f8a-4d9e-a818-20ead899ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rerank_query = response['query']\n",
    "no_rerank_response = response['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0ed159-11e3-4be7-b00b-76ac11c7b672",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090bfc21-4e16-4859-883a-052713900798",
   "metadata": {},
   "source": [
    "## Do Reranking with CohereRerank\n",
    "\n",
    "Wrap base retriever with a `ContextualCompressionRetriever`. Add an `CohereRerank`, uses the Cohere rerank endpoint to rerank the returned results. Do note that it is mandatory to specify the model name in CohereRerank!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eedabd-e66d-4e4f-8f11-a544ef7dcb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "#\n",
    "compressed_docs = compression_retriever.get_relevant_documents(query)\n",
    "\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f926e957-dc94-4d74-8f0f-e3196815cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=compression_retriever \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e4f93d-5527-4421-8865-650c1e4119dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "response = qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022991c7-e819-4613-8a3e-bd696efd88dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query = {response['query']}\")\n",
    "print(f\"Result = {response['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2926333d-b1b7-45ab-ba29-382315fd9c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_query = response['query']\n",
    "rerank_response = response['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1fbb5a-69fb-4495-92bb-a6344621b20c",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "Do you think the reranked response is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c9d67-01c4-4e6d-bf9d-8d3eb2cd74c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'Query': [no_rerank_query, rerank_query], \n",
    "        'Response': [no_rerank_response, rerank_response]\n",
    "    },\n",
    "    index=([\"Not Rerank\", \"Reranked\"])\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b090d-3505-4660-8271-b8ccbe137af1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf88827-db49-4d7d-86cb-a5c1fb81cb8d",
   "metadata": {},
   "source": [
    "# Another Example Using Infinity Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df354832-6a0d-417b-825e-bf365462cb8c",
   "metadata": {},
   "source": [
    "# Infinity Reranker\n",
    "\n",
    "Source: [LangChain Infinity Reranker](https://python.langchain.com/docs/integrations/document_transformers/infinity_rerank/)\n",
    "\n",
    "`Infinity` is a high-throughput, low-latency REST API for serving text-embeddings, reranking models and clip.\n",
    "For more info, please visit [here](https://github.com/michaelfeil/infinity?tab=readme-ov-file#reranking).\n",
    "\n",
    "This notebook shows how to use Infinity Reranker for document compression and retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7ef7f7-e3a2-4856-9e57-86f88e5b745b",
   "metadata": {},
   "source": [
    "You can launch an Infinity Server with a reranker model in CLI (Command Line Interface):\n",
    "\n",
    "```bash\n",
    "pip install \"infinity-emb[all]\"\n",
    "infinity_emb v2 --model-id mixedbread-ai/mxbai-rerank-xsmall-v1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7961d8d-3298-402b-93c2-06558bc18645",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "\n",
    "%pip install --quiet -U \"infinity-emb[all]\"\n",
    "%pip install --quiet -U infinity_client\n",
    "%pip install --quiet -U faiss-cpu\n",
    "%pip install --quiet -U langchain-community\n",
    "%pip install --quiet -U langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d4f5aa-298a-4080-9675-407fd1f782dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install colab-xterm\n",
    "%load_ext colabxterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a635fd2-2507-426e-9370-b59d41efc4be",
   "metadata": {},
   "source": [
    "## xterm \n",
    "\n",
    "The command in the cell will run \"xterm\".  \n",
    "\n",
    "Cut and paste the command string `infinity_emb v2 --model-id mixedbread-ai/mxbai-rerank-xsmall-v1` into the terminal.\n",
    "\n",
    "<img src=\"ScreenShot_02.png\" width=\"auto\" height=\"auto\">     \n",
    "\n",
    "Wait for the command to complete its run. It might take a while. Look out for `Application startup complete`.  Take note of the http URL and port number.  In the screen shot shown, it is `http://0.0.0.0:7997`.\n",
    "\n",
    "<img src=\"ScreenShot_01.png\" width=\"auto\" height=\"auto\">                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5361c8b-69d7-4c57-b9d4-e40683d3b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e385a-9799-46a5-a497-905049fa5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for printing docs\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b2b6a-3c4d-442a-b325-51264733a156",
   "metadata": {},
   "source": [
    "## Set up the base vector store retriever\n",
    "Let's start by initializing a simple vector store retriever and storing the 2023 State of the Union speech (in chunks). We can set up the retriever to retrieve a high number (20) of docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310c5d5-63e1-4153-9092-8fb30a0bad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f829b4e-858c-4b00-8dbb-1e2d7c4efd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/koayst-rplesson/SDGAI_LLMforGenAIApp_Labs/raw/refs/heads/main/L15/state_of_the_union.txt\n",
    "!ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e47616-a9fd-42ac-be74-efe800334fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = TextLoader(\"./state_of_the_union.txt\").load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "retriever = FAISS.from_documents(\n",
    "    texts, HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    ").as_retriever(search_kwargs={\"k\": 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a3b774-8b96-4073-aef6-938388f1c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = retriever.invoke(query)\n",
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf675062-0b88-472c-9462-7e6a3a6e263e",
   "metadata": {},
   "source": [
    "## Reranking with InfinityRerank\n",
    "Now let's wrap our base retriever with a `ContextualCompressionRetriever`. We'll use the `InfinityRerank` to rerank the returned results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ee9f67-48f5-4cd3-a415-11b41fcf9639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from infinity_client import Client\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.document_compressors.infinity_rerank import InfinityRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4698d9bb-abc1-45c3-a8cf-893bf4f2681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify base_url to the http URL noted earlier\n",
    "base_url = \"http://0.0.0.0:7997\"\n",
    "\n",
    "client = Client(base_url=base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad00893-cc88-4b17-b2b7-1beb412bf8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \"mixedbread-ai/mxbai-rerank-xsmall-v1\" is downloaded from Hugging Face repository\n",
    "compressor = InfinityRerank(client=client, model=\"mixedbread-ai/mxbai-rerank-xsmall-v1\")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b643cd-c353-4792-89ab-91eb7fbd39a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"What did the president say about Ketanji Jackson Brown\"\n",
    ")\n",
    "\n",
    "pretty_print_docs(compressed_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
